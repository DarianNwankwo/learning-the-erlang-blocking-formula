{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efa41f0e-d799-4683-b468-a12da312cf41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import statistics as stat\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import array\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "def Heavy_Traffic(arr_rate, ser_rate,arr_var, ser_var,C):\n",
    "    rho = arr_rate/ser_rate\n",
    "    eta = 1/2\n",
    "    c = 1\n",
    "    z = 1 + (c**2 -1)*eta\n",
    "    Block = math.sqrt(z/rho)*scipy.stats.norm(0, 1).pdf((C-rho)/math.sqrt(rho*z))/scipy.stats.norm(0, 1).cdf((C-rho)/math.sqrt(rho*z))\n",
    "    return Block\n",
    "\n",
    "def Hayward(arr_rate, ser_rate, arr_var, ser_var, C):\n",
    "    rho = arr_rate/ser_rate\n",
    "    eta = 1/2\n",
    "    c = 1\n",
    "    z = 1 + (c**2 -1)*eta\n",
    "    f1 = scipy.special.gammainc((C+1)/z, rho/z)\n",
    "    f2 = scipy.special.gamma((C+1)/z)\n",
    "    f3 = scipy.special.gammainc(C/z, rho/z)\n",
    "    f4 = scipy.special.gamma(C/z)\n",
    "    Block = (f1/f2 - f3/f4)*f2/f1\n",
    "\n",
    "def Erlang_B(arr_rate,ser_rate,C):\n",
    "    rho = arr_rate/ser_rate\n",
    "    Blocking_Probability = 1\n",
    "    for i in range(C+1):\n",
    "        Blocking_Probability = rho*Blocking_Probability/(rho*Blocking_Probability + i)\n",
    "    return Blocking_Probability\n",
    "\n",
    "def collect_data(uni_a,uni_b,uni_c, uni_d,C,N): #C = number of servers, N = number of data points,\n",
    "    Block = np.ones(N)\n",
    "    Block2 = np.ones(N)\n",
    "    Block3 = np.ones(N)\n",
    "    uni_vec_1 = np.random.uniform(uni_a,uni_b,N)  \n",
    "    uni_vec_2 = np.random.uniform(uni_c,uni_d,N)\n",
    "    uni_vec_3 = np.random.randint(1,C,N)\n",
    "    for i in range(N):\n",
    "        Block[i] = Erlang_B(uni_vec_1[i],uni_vec_2[i],uni_vec_3[i])\n",
    "        Block2[i] = Heavy_Traffic(uni_vec_1[i],uni_vec_2[i],uni_vec_1[i],uni_vec_2[i],uni_vec_3[i])\n",
    "        Block3[i] = Hayward(uni_vec_1[i],uni_vec_2[i],uni_vec_1[i],uni_vec_2[i],uni_vec_3[i])\n",
    "    return uni_vec_1,uni_vec_2,uni_vec_3,Block,Block2,Block3\n",
    "\n",
    "def simulate_queue(N,K,arrival,service):\n",
    "    queue_upon_arrival = np.zeros(N);\n",
    "    blocked = np.zeros(N);\n",
    "    blocked2 = np.zeros(N);\n",
    "    arrival_times = np.cumsum(arrival);\n",
    "    departure_times =  np.zeros(N);\n",
    "\n",
    "# Computing the queue length upon arrival\n",
    "    queue_upon_arrival[0] = 0;\n",
    "    blocked[0] = 0;\n",
    "    blocked2[0] = 0;\n",
    "    new_index = 0\n",
    "    num_departed = 0\n",
    "    departure_times[0] = arrival_times[0] + service[0];\n",
    "    for i in range(new_index,N-1):\n",
    "        count = 0 ;\n",
    "        #print(new_index)\n",
    "        for j in range(1,i):\n",
    "            num_departed = num_departed + (departure_times[j] < arrival_times[i+1])\n",
    "            count = count + (departure_times[j] - arrival_times[i] > 0)\n",
    "   \n",
    "        queue_upon_arrival[i] = count\n",
    "        new_index = int(np.max(num_departed - blocked2[i] - K,0))\n",
    "        departure_times[i] = arrival_times[i] + service[i]*(count < K )\n",
    "        blocked[i] = (count < K)\n",
    "        blocked2[i] = blocked2[i-1] + (1-blocked[i])\n",
    "    mean_blocked = 1-np.mean(blocked)\n",
    "    mean_queue = np.mean(queue_upon_arrival)\n",
    "    return mean_blocked,mean_queue\n",
    "\n",
    "  # Generating the Data  \n",
    "N=10000\n",
    "C=100\n",
    "uni_vec_1,uni_vec_2,uni_vec_3,Block,Block2,Block3 = collect_data(1,100,1,10,C,N)\n",
    "m = 5\n",
    "X = np.column_stack((uni_vec_1, uni_vec_2))\n",
    "#X = np.column_stack((X, rho))\n",
    "test_array = Block[N-m:N]\n",
    "#actual_array = mean_sys[N-m,N]\n",
    "rho = np.divide(uni_vec_1, uni_vec_2)\n",
    "\n",
    "\n",
    "l_linear_model = LinearRegression().fit(X[0:N-m], Block[0:N-m])\n",
    "l_prediction_array = []\n",
    "for i in range(m):\n",
    "    l_prediction = l_linear_model.intercept_ + l_linear_model.coef_[0]*X[N-m+i][0] \\\n",
    "    + l_linear_model.coef_[1]*X[N-m+i][1]\n",
    "    l_prediction_array.append(l_prediction)\n",
    "#print('l actual: ', l_actual_array)\n",
    "#print('l predict: ', l_prediction_array)\n",
    "#print('intercept:', l_linear_model.intercept_)\n",
    "#print('slope:', l_linear_model.coef_)\n",
    "means_squared_error_l = math.pow(np.linalg.norm(test_array-l_prediction_array), 2)/m\n",
    "print('means_squared_error for l: ', means_squared_error_l)\n",
    "print(l_linear_model.intercept_, l_linear_model.coef_)\n",
    "print('rho values: ', rho[N-m:N])\n",
    "print('test data: ', test_array)\n",
    "print('heavy_traffic: ', Block2[N-m:N])\n",
    "print('lin reg predict: ', l_prediction_array)\n",
    "#print(X[N-m:N])\n",
    "#print('shape of Y', Block.shape)\n",
    "#print(arrival_rate)\n",
    "#print('shape of X', X.shape)\n",
    "print(X)\n",
    "Nstar = 4000\n",
    "M=5\n",
    "blocking = np.zeros(M)\n",
    "queue_length = np.zeros(M)\n",
    "for i in range(M):\n",
    "    lam = 1/X[N-m+i][0]\n",
    "    mu = 1/X[N-m+i][1]\n",
    "    print(i)\n",
    "    blocking[i],queue_length[i] = simulate_queue(Nstar,C,np.random.exponential(scale=lam, size=Nstar),np.random.exponential(scale=mu, size=Nstar))\n",
    "print('simulated blocking probability:', blocking)\n",
    "\n",
    "#  # deep neural network\n",
    "#     # ReLU: 100 layer\n",
    "# n_relu100 = MLPRegressor(hidden_layer_sizes=100, activation='relu', max_iter=10000).fit(X[0:N-m], Block[0:N-m])\n",
    "# n_relu100_prediction = n_relu100.predict(X[N-m:N])\n",
    "\n",
    "\n",
    "# print('\\nrelu100 predict: ', n_relu100_prediction)\n",
    "# AMSE_n_relu100 = math.pow(np.linalg.norm(test_array-n_relu100_prediction), 2) / m\n",
    "# AMSE_relu100 = []\n",
    "# AMSE_relu100.append(AMSE_n_relu100)\n",
    "# print('\\nerror for relu100: ', AMSE_n_relu100)\n",
    "\n",
    "\n",
    "\n",
    "# # deep neural network\n",
    "#     # ReLU: 100 layer\n",
    "# n_relu100 = MLPRegressor(hidden_layer_sizes=100, activation='relu', max_iter=10000).fit(DataSet[0:S-m], littles_law[0:S-m])\n",
    "# n_relu100_prediction = n_relu100.predict(DataSet[S-m:S])\n",
    "# #print('\\nrelu100 predict: ', n_relu100_prediction)\n",
    "# AMSE_n_relu100 = math.pow(np.linalg.norm(actual_array-n_relu100_prediction), 2) / m\n",
    "# AMSE_relu100 = []\n",
    "# AMSE_relu100.append(AMSE_n_relu100)\n",
    "# print('\\nerror for relu100: ', AMSE_n_relu100)\n",
    "\n",
    "# # deep neural network\n",
    "#     # ReLU: 400 layer\n",
    "# n_relu400 = MLPRegressor(hidden_layer_sizes=400, activation='relu', max_iter=10000).fit(DataSet[0:S-m], littles_law[0:S-m])\n",
    "# n_relu400_prediction = n_relu400.predict(DataSet[S-m:S])\n",
    "# #print('\\nrelu400 predict: ', n_relu400_prediction)\n",
    "# AMSE_n_relu400 = math.pow(np.linalg.norm(actual_array-n_relu400_prediction), 2) / m\n",
    "# AMSE_relu400 = []\n",
    "# AMSE_relu400.append(AMSE_n_relu400)\n",
    "# print('\\nerror for relu400: ', AMSE_n_relu400)\n",
    "\n",
    "#  # deep neural network\n",
    "#     # TANH: 100 layer\n",
    "# n_tanh100 = MLPRegressor(hidden_layer_sizes=100, activation='tanh', max_iter=10000).fit(DataSet[0:S-m], littles_law[0:S-m])\n",
    "# n_tanh100_prediction = n_tanh100.predict(DataSet[S-m:S])\n",
    "# #print('\\ntanh100 predict: ', n_tanh100_prediction)\n",
    "# AMSE_n_tanh100 = math.pow(np.linalg.norm(actual_array-n_tanh100_prediction), 2) / m\n",
    "# AMSE_tanh100 = []\n",
    "# AMSE_tanh100.append(AMSE_n_tanh100)\n",
    "# print('\\nerror for tanh100: ', AMSE_n_tanh100)\n",
    "\n",
    "\n",
    "\n",
    "#  # deep neural network\n",
    "#     # SIG: 100 layer\n",
    "# n_sig100 = MLPRegressor(hidden_layer_sizes=100, activation='logistic', max_iter=10000).fit(DataSet[0:S-m], littles_law[0:S-m])\n",
    "# n_sig100_prediction = n_sig100.predict(DataSet[S-m:S])\n",
    "# #print('\\nsig100 predict: ', n_sig100_prediction)\n",
    "# AMSE_n_sig100 = math.pow(np.linalg.norm(actual_array-n_sig100_prediction), 2) / m\n",
    "# AMSE_sig100 = []\n",
    "# AMSE_sig100.append(AMSE_n_sig100)\n",
    "# print('\\nerror for sig100: ', AMSE_n_sig100)\n",
    "\n",
    "\n",
    "# #knn regression model k = 5\n",
    "# k_prediction_array_5 = []\n",
    "# for i in range(m):\n",
    "#     test_point = np.array(littles_law[S-m+i])\n",
    "#     k_nearest_neighbors, k_prediction_5 = knn(Z, test_point, k=5, distance_fn=euclidean_distance, choice_fn=mean)\n",
    "#     k_prediction_array_5.append(k_prediction_5)\n",
    "# #print('k actual: ', littles_law[S-m:S])\n",
    "# #print('k predict: ', k_prediction_array_5)\n",
    "# means_squared_error_k_5 = math.pow(np.linalg.norm(actual_array-k_prediction_array_5), 2)/m\n",
    "# print('means_squared_error for k=5: ', means_squared_error_k_5)\n",
    "\n",
    "# #knn regression model k = 10\n",
    "# k_prediction_array_10 = []\n",
    "# for i in range(m):\n",
    "#     test_point = np.array(littles_law[S-m+i])\n",
    "#     k_nearest_neighbors_10, k_prediction_10 = knn(Z, test_point, k=10, distance_fn=euclidean_distance, choice_fn=mean)\n",
    "#     k_prediction_array_10.append(k_prediction_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bac8c4d-d5c1-40c3-98de-92724ea92fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6fa2e-3ed4-4832-b8e8-17eedd589c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
